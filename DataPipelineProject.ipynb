{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMgpdWs35ZBWmUix5Xzr27B",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VarunMani04/Datapipeline-Project/blob/main/DataPipelineProject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podcast Parameters & Logic"
      ],
      "metadata": {
        "id": "pogxGl2RjJX2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Podcast: Marketplace Tech\n",
        "Link: \"https://www.marketplace.org/feed/podcast/marketplace-tech\"\n",
        "\n",
        "from airflow.decorators import dag, task\n",
        "import pendulum\n",
        "\n",
        "import requests #xml podcast data processor\n",
        "import xmltodict\n",
        "\n",
        "@dag(\n",
        "    start_date = pendulum.datetime(2023,9,1)\n",
        "    dag_id = \"podcast_summary\"\n",
        "    schedule_interval = \"@daily\" #collected daily\n",
        "    catchup = False\n",
        ")"
      ],
      "metadata": {
        "id": "hNsmkG6bjUun"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating a Database"
      ],
      "metadata": {
        "id": "xosWH4P0s38Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from airflow.providers.sqlite.operators.sqlite import SqliteOperator\n",
        "\n",
        "  def podcast_summary():\n",
        "    create_database = SqliteOperator( #Pass in SQL Code\n",
        "        task_id='create_table_sqlite',\n",
        "        sql=r\"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS episodes (\n",
        "            link TEXT PRIMARY KEY,\n",
        "            title TEXT,\n",
        "            filename TEXT,\n",
        "            published TEXT,\n",
        "            description TEXT,\n",
        "            transcript TEXT\n",
        "        );\n",
        "        \"\"\",\n",
        "        sqlite_conn_id=\"podcasts\" #tells airflow which connection to use\n",
        "    )\n",
        "    @task()\n"
      ],
      "metadata": {
        "id": "e-BUchsVsp2B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Downloading Data"
      ],
      "metadata": {
        "id": "aeGycBOet0Zu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def podcast_logic(): #base logic for pipeline\n",
        "  @task()\n",
        "\n",
        "  def get_podcasts(Link):\n",
        "    data = requests.get()\n",
        "    feed = xmltodict.parse(data.text)\n",
        "    names = feed[\"rss\"][\"channel\"][\"item\"] #parses through dictionary to names\n",
        "    return (\"Episode names: \", names )\n",
        "\n",
        "\n",
        "  pipeline = podcast_logic() #pipeline object initalized\n",
        "  episodes = get_podcasts():\n",
        "  create_databse.set_downstream(episodes)"
      ],
      "metadata": {
        "id": "lFrXcqsPtaaH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Episodes into Database Avoiding Duplicates"
      ],
      "metadata": {
        "id": "4SrNwEUvwg6z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from airflow.providers.sqlite.hooks.sqlite import SqliteHook\n",
        "\n",
        "def load_episodes(episodes):\n",
        "  hook = SqliteHook(splite_conn_id=\"podcasts\")\n",
        "  stored = hook.get_pandas_df(\"SELECT * from episodes;\") #figure out what episodes have been stored\n",
        "  new_episodes = [] #if not add to database\n",
        "\n",
        "  for episode in episodes: #process through each episode\n",
        "    if episode[\"link\"] not in stored[\"link\"].values:\n",
        "      filename = f\"{episode[\"link\"].split(\"/\"[-1])}.mp3\"\n",
        "      new_episodes.append([episode[\"link\"], episode[\"title\"], episode[\"pubDate\"], episode[\"description\"], filename])\n",
        "  hook.insert_rows(table='episodes', rows=new_episodes, target_fields=[\"link\", \"title\", \"published\", \"description\", \"filename\"]) #Each new episode will have these parameters\n",
        "\n",
        "load_episodes(episodes)"
      ],
      "metadata": {
        "id": "pNgpP6yauvPx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download Episodes"
      ],
      "metadata": {
        "id": "EK8Pdg0exCRm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "@task() #Airflow feature that automates function tasks\n",
        "def download_episodes(episodes):\n",
        "  for episode in episodes:\n",
        "    filename = f\"{episode[\"link\"].split(\"/\")[-1]}.mp3\" #fstring to convert to mp3\n",
        "    audio = os.path.join()\n",
        "\n",
        "finalSum = podcast_summary()"
      ],
      "metadata": {
        "id": "vZ7EApr9xBog"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}